
import jueru

import gym
import numpy as np
import torch
from jueru.Agent_set import Agent, Sac_agent
from jueru.algorithms import BaseAlgorithm, SACAlgorithm
from jueru.datacollection import Replay_buffer
from jueru.updator import critic_updator_ddpg, actor_updator_ddpg, soft_update, actor_and_alpha_updator_sac, \
    critic_updator_sac
from jueru.user.custom_actor_critic import MLPfeature_extractor, ddpg_actor, ddpg_critic, FlattenExtractor, Sac_actor, \
    Sac_critic

from reward_machines_all.envs.mujoco_rm.half_cheetah_environment import MyHalfCheetahEnvRM1
from reward_machines_all.reward_machines.rm_environment import RewardMachineWrapper

env = MyHalfCheetahEnvRM1()
env = RewardMachineWrapper(env, add_crm=True, add_rs=False, gamma=0.95, rs_gamma=0.90)
print(env.observation_space)
feature_extractor = FlattenExtractor(env.observation_space)
#np.prod(env.observation_space.shape)
actor = Sac_actor(
    action_space=env.action_space, hidden_dim=256, feature_extractor=feature_extractor,
    feature_dim=np.prod(env.observation_space.shape), log_std_min=-10, log_std_max=2
)

critic = Sac_critic(
    action_space=env.action_space, feature_extractor=feature_extractor, hidden_dim=256,
    feature_dim=np.prod(env.observation_space.shape)
)

log_alpha = torch.tensor(np.log(0.01))
log_alpha.requires_grad = True

data_collection_dict = {}

data_collection_dict['replay_buffer'] = Replay_buffer(env=env, size=3e6)

functor_dict = {}

lr_dict = {}

updator_dict = {}

functor_dict['actor'] = actor

functor_dict['critic'] = critic

functor_dict['critic_target'] = None

functor_dict['log_alpha'] = log_alpha

lr_dict['actor'] = 1e-3

lr_dict['critic'] = 1e-3

lr_dict['critic_target'] = 1e-4

lr_dict['log_alpha'] = 1e-4

updator_dict['actor_and_alpha_update'] = actor_and_alpha_updator_sac

updator_dict['critic_update'] = critic_updator_sac

updator_dict['soft_update'] = soft_update


class CRMSACAlgorithm(SACAlgorithm):
    def learn(self, num_train_step, actor_update_freq, reward_scale=1):
        self.actor_update_freq = actor_update_freq

        step = 0

        while step <= (num_train_step):

            state = self.env.reset()
            episode_reward = 0
            episode_step = 0
            while True:

                if self.render:
                    self.env.render()

                if step < self.start_steps:
                    action = self.env.action_space.sample()
                else:
                    with torch.no_grad():
                        action = self.agent.sample_action(state)

                next_state, reward, done, info = self.env.step(action)
                # print('info', info)
                experiences = info["crm-experience"]

                reward = reward_scale * reward

                episode_step += 1
                if self.max_episode_steps:
                    if episode_step == self.max_episode_steps:
                        self.writer.add_scalar('episode_reward_step', episode_reward, global_step=step)
                        episode_reward = 0
                        episode_step = 0
                        done = True
                done_value = 0 if done else 1

                for _obs, _action, _r, _new_obs, _done in experiences:
                    done_value = 0 if _done else 1
                    _r = _r * reward_scale
                    self.data_collection_dict['replay_buffer'].store(_obs, _action, _r, _new_obs, done_value)
                # ('state', 'action', 'reward', 'next_state', 'mask', 'log_prob')

                state = next_state

                episode_reward += reward

                if step >= self.min_update_step and step % self.update_step == 0:
                    for _ in range(self.update_step):
                        batch = self.data_collection_dict['replay_buffer'].sample_batch(self.batch_size)

                        self.updator_dict['critic_update'](self.agent, obs=batch['state'], action=batch['action'],
                                                           reward=batch['reward'], next_obs=batch['next_state'],
                                                           not_done=batch['done'], gamma=self.gamma)
                        self.updator_dict['actor_and_alpha_update'](self.agent, obs=batch['state'],
                                                                    target_entropy=-self.agent.functor_dict[
                                                                        'critic'].action_dim)

                        self.updator_dict['soft_update'](self.agent.functor_dict['critic_target'].Q1,
                                                         self.agent.functor_dict['critic'].Q1,
                                                         polyak=self.polyak)
                        self.updator_dict['soft_update'](self.agent.functor_dict['critic_target'].Q2,
                                                         self.agent.functor_dict['critic'].Q2,
                                                         polyak=self.polyak)

                step += 1
                if step >= self.min_update_step and step % self.save_interval == 0:
                    self.agent.save(address=self.model_address)
                if done:

                    break


sac = CRMSACAlgorithm(agent_class=Sac_agent,
                      functor_dict=functor_dict,
                      lr_dict=lr_dict,
                      updator_dict=updator_dict,
                      data_collection_dict=data_collection_dict,
                      env=env,
                      gamma=0.99,
                      batch_size=300,
                      tensorboard_log="./CRM_SAC_tensorboard",
                      render=False,
                      action_noise=0.1,
                      min_update_step=1000,
                      update_step=100,
                      polyak=0.995,
                      save_interval=2000,
                      max_episode_steps=1000,
                      )

sac.learn(num_train_step=3000000, actor_update_freq=2, reward_scale=1)
