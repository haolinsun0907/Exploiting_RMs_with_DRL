# Exploiting Reward Machines with Deep Reinforcement Learning
# Abstract

In this project, we address the challenges of partial observability and learning efficiency in deep reinforcement learning (DRL) by extending the capabilities of reward machines (RMs) and counterfactual experiences for reward machines (CRM). RM and CRM were proposed by Toro Icarte et al. \cite{rm_icml}. A reward machine can decompose a task, convey its high-level structure to an agent, and support certain non-Markovian task specifications. Our objective is to identify and integrate state-of-the-art DRL algorithms with RMs to enhance learning efficiency. Our experimental results demonstrate that Soft Actor-Critic with counterfactual experiences for RMs (SAC-CRM) facilitates faster learning of better policies, while Deep Deterministic Policy Gradient with counterfactual experiences for RMs (DDPG-CRM) is slower, less efficient, but more stable. Option-based Hierarchical Reinforcement Learning for reward machines (HRM) and Twin Delayed Deep Deterministic (TD3) with CRM generally underperform compared to SAC-CRM and DDPG-CRM. This work contributes to the ongoing development of more efficient and robust DRL approaches by leveraging the potential of RMs in real-world problem-solving scenarios.

A complete description of reward machines and our methods can be found in the following paper ([link](https://arxiv.org/abs/2010.03950)):

    @article{tor-etal-arxiv20,
        author  = {Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
        title   = {Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning},
        journal = {arXiv preprint arXiv:2010.03950},
        year    = {2020}
    }

Our methods build on top of the following works on reward machines ([icml18](http://proceedings.mlr.press/v80/icarte18a.html), [ijcai19](https://www.ijcai.org/Proceedings/2019/840)):

    @inproceedings{tor-etal-icml18,
        author    = {Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
        title     = {Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning},
        booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
        year      = {2018},
        pages     = {2112--2121}
    }
    @inproceedings{cam-etal-ijcai19,
        author    = {Camacho, Alberto and Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
        title     = {{LTL} and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning},
        booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
        year      = {2019},
        pages     = {6065--6073}
    }

This code is meant to be a usable version of our approach. If you find any bugs or have questions about it, please let us know. We'll be happy to help you!


## Installation instructions

The code has the following requirements: 

- Python 3.6 or 3.7
- PyTorch
- NumPy
- OpenAI Gym
- [OpenAI Baselines](https://github.com/openai/baselines)

However, the only *real* requirement is to have installed the master branch of Baselines. Installing baselines is not trivial, though. Their master branch only supports Tensorflow from version 1.4 to 1.14. These versions of Tensorflow seem to work fine with Python 3.6 and 3.7, but they **do not** work with Python 3.8+. We also included a [requirements.txt](requirements.txt) file as a reference, but note that such a file includes more libraries than the ones strictly needed to run our code.


## How to run the code

Because we used different tools to develop the algorithms (**PyTorch** for SAC-CRM and **TensorFlow** for the rest algorithms), the way to run them is a little bit different.

### To run SAC-CRM and baseline SAC

To run the code of SAC-CRM, move to the *SAC-CRM* folder and execute *crm_sac_test.py*

To run the code of baseline SAC, move to the *SAC-CRM* folder and execute *sac_test.py*

### To run other algorithms (TD3-CRM, baseline TD3, DDPG-CRM, baseline DDPG, PPO-CRM, baseline PPO)

To run the code of the rest algorithms, move to the *reward_machines* folder and execute *run.py*. This code follows the same interface as *run.py* from [Baselines](https://github.com/openai/baselines):

```
python3 run.py --alg=<name of the algorithm> --env=<environment_id> [additional arguments]
```

However, we included the following (additional) RM-tailored algorithms (which are described in the paper). Some can be used as the value of the `--alg=` flag and some are activated with additional arguments.

- **Counterfactual Experiences for Reward Machines (CRM)**: CRM is an RM-tailored approach that uses counterfactual reasoning to generate *synthetic* experiences in order to learn policies faster. CRM can be combined with any off-policy learning method, including TD3 (`--alg=td3`) and deep deterministic policy gradient (`--alg=ddpg`). To use CRM, include the flag `--use_crm` when running an experiment.

- **Hierarchical RL for Reward Machines (HRM)**: HRM is an RM-tailored approach that automatically extracts a set of *options* from a RM to learn policies faster. We included implementations of tabular HRM (`--alg=hrm`) and deep HRM (`--alg=dhrm`). Note that `dhrm` can learn the option policies using DQN or DDPG and the macro-controller is learned using DQN. In addition to the standard learning hyperparameters, HRM uses R_min to penalize an option policy when it reaches an unwanted subgoal (this can be set with, e.g., `--r_min=-1`), R_max to reward an option policy when it reaches its target subgoal (e.g., `--r_max=1`), and another hyperparameter to define whether to learn options for the self-loops (by default, HRM does not learn options for self-loops unless the flag `--use_self_loops` is present).

- **Automated Reward Shaping (RS)**: RS is an RM-tailored approach that changes the reward from a *simple RM* so that the optimal policies remain the same but the overall reward becomes less sparse. RS can be used with tabular Q-learning (`--alg=qlearning`), deep Q-networks (`--alg=deepq`), deep deterministic policy gradient (`--alg=ddpg`), tabular HRM (`--alg=hrm`), and deep HRM (`--alg=dhrm`). To use RS, include the flag `--use_rs` when running an experiment. Note that RS uses two hyperparameters in determining the shaped rewards -- the same discount factor used for the environment (which can be set with, e.g., `--gamma=0.9`) and the discount factor used in calculating the potential function (e.g., `--rs_gamma=0.9`). 

For each of the different algorithms that can be named in the `--alg=` flag, default values for the hyperparameters in various environments are specified in `reward_machines/rl_agents/<name of the algorithm>/defaults.py`. For example, there are hyperparameters for deep HRM in [reward_machines/rl_agents/dhrm/defaults.py](reward_machines_all/rl_agents/dhrm/defaults.py), which specify (among other things) that the option polices are learned using DQN in the *water domain* and DDPG in the *half-cheetah domain*.

Note that RM-tailored algorithms assume that the environment is a *RewardMachineEnv* (see [reward_machines/reward_machines/rm_environment.py](reward_machines_all/reward_machines/rm_environment.py)). These environments define their reward function using a reward machine. We included the following RM environments in our code:

- **Half-Cheetah**: The half-cheetah domain includes single task settings and 4 different tasks. Use `--env=Half-Cheetah-RM1-v0` to run the task and replace the task description with other tasks to run the rest of the tasks. Both tasks are discussed in the paper.
- **Ant**: The ant domain includes single task settings and 3 different tasks. Use `--env=Ant-RM1-v0` to run the task and replace the task description with other tasks to run the rest of the tasks. Both tasks are discussed in the paper.

Finally, note that adding more RM environments is farily straightforward. The *RewardMachineEnv* interface receives any gym environment (*gym.Env*) and paths to files that define the reward machines. It also requires implementing the `get_events(...)` function. That function returns the events that currently hold in the environment. If you would like to include a new RM environment, we recommend you to follow a similar structure to [reward_machines/envs/mujoco_rm/half_cheetah_environment.py](reward_machines_all/envs/mujoco_rm/half_cheetah_environment.py). That is a lean example where we adapt the well-known *HalfCheetahEnv* environment to solve tasks using RMs.



